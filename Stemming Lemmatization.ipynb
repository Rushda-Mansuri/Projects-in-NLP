{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pccWC5dHHUMj"
      },
      "source": [
        "# <font color='#00d2d3'>  NLTK: <br> Stemming: Porter Stemmer, Lancaster Stemmer and Snowball <br> Lemmatization<font/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTZmo5ujIHqi"
      },
      "source": [
        "<font color='#e84393' > Strip affixes from the token and return the stem. <br/> token (str) – The token that should be stemmed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYfRvXwffWC3"
      },
      "outputs": [],
      "source": [
        "#import the nltk package\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMx0DDNbS8pd"
      },
      "source": [
        "# <font color='#00d2d3' > 1. Porter Stemmer Algorithm - “An algorithm for suffix stripping.” <br/> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfA7ew8ALkLb"
      },
      "source": [
        "One of the most widely used stemming algorithms is the simple and efficient Porter algorithm, which is based on a series of simple cascaded rewrite rules.  The algorithm contains a series of rules like these: <br/>\n",
        "<font color='#e84393' > ATIONAL —►ATE (e.g., relational —►relate)<br/>\n",
        "ING —>6 ifstemcontainsvowel(e.g.,motoring—►motor)<br/>\n",
        "SSES —►SS (e.g., grasses —►grass) <br> </font> \n",
        "Commonly used, gentle algorithm but is not the best for precision. \n",
        "For more information on the Porter Stemmer Algorithm \n",
        "https://tartarus.org/martin/PorterStemmer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxaa-qsmgApb"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sry5f3bgIty"
      },
      "outputs": [],
      "source": [
        "porter = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQmQb6HWgVB0",
        "outputId": "ecc07331-8ce9-4420-de5d-0ebd0e7958ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bundl\n"
          ]
        }
      ],
      "source": [
        "print(porter.stem('bundles'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJQmcHJXga39",
        "outputId": "a4598cb1-c8d5-4719-d039-4175da0b0d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token\n"
          ]
        }
      ],
      "source": [
        "print(porter.stem('tokenization'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQfbUOXohaoP",
        "outputId": "3c170059-8fb7-4b66-908a-0870d37b932c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "illustr\n"
          ]
        }
      ],
      "source": [
        "print(porter.stem('Illustrator'))    #since this is already a noun, stemming it does not help. we will address this later on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECzgMP9uVWWG"
      },
      "source": [
        "# <font color='#00d2d3' > 2. Lancaster - A word stemmer based on the Lancaster stemming algorithm <br/> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mIdxTChQVcl"
      },
      "source": [
        "<font color='#00d2d3' > \n",
        "An iterative algorithm with rules. About 120 rules are indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats. <br/> It is an aggressive algorithm that can give low precision. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQHYjyXehIEW"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LBK2VnidEPp",
        "outputId": "1562fe45-10f6-485b-d577-45489e2e7f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "org\n"
          ]
        }
      ],
      "source": [
        "print(lancaster.stem('organization'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkD-ywFchRhG",
        "outputId": "a10724bc-5136-43b8-8cde-7875abfc583d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "organ\n"
          ]
        }
      ],
      "source": [
        "print(porter.stem('organization'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT3a3vf9dSNg"
      },
      "source": [
        "Shows how Lancaster is more aggressive.\n",
        "It not only strips the suffix but also adds proper suffix to the stem to give it meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj3XWBIAhgcg",
        "outputId": "7cc70d35-e203-42ca-90ca-b79289d632b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bunny\n"
          ]
        }
      ],
      "source": [
        "print(lancaster.stem('Bunnies'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6dJ0YCMdcL7",
        "outputId": "4f124abb-76b5-4994-9fe2-c197456ffa88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bunni\n"
          ]
        }
      ],
      "source": [
        "print(porter.stem('Bunnies'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLf2TQ1MdooC",
        "outputId": "8d885090-dc9f-4071-af9a-6d04fd5a6d27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "illust\n"
          ]
        }
      ],
      "source": [
        "print(lancaster.stem('Illustrator'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUe8eiioWg5o"
      },
      "source": [
        "# <font color='#00d2d3' > 3. Snowball -  <br/> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XsypTPFXFlU"
      },
      "source": [
        "<font color='#00d2d3' > 3. Snowball - A modified version of the Porter Stemmer. Developed by the same guy. More precise for large datasets. <br/> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7yvA1dDd0Fo"
      },
      "source": [
        "We might not see much difference but it has higher precision than Porter Stemmer for large datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4bUHLzQh8Z8"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snow = SnowballStemmer('english')             #can stem for multiple languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmzyTEzViR0o",
        "outputId": "60e6a316-53a8-4940-f3e1-89b8989ee468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "organ\n",
            "digit\n",
            "favour\n",
            "limit\n",
            "illustr\n"
          ]
        }
      ],
      "source": [
        "print(snow.stem('Organization'))\n",
        "print(snow.stem('Digitize'))\n",
        "print(snow.stem('favourable'))\n",
        "print(snow.stem('limitation'))\n",
        "print(snow.stem('Illustrator'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mziGnrNgevis"
      },
      "source": [
        "# Comparing the three methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09_ZD9_1UFI9",
        "outputId": "271b08b8-9b52-4b0b-db86-3a8f2ed284f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bunnies', 'organization', 'polarize', 'jaguar', 'stabalize', 'destabilize', 'democratic', 'kingdoms', 'dramatic', 'favourable']\n"
          ]
        }
      ],
      "source": [
        "bundle = ['bunnies','organization','polarize','jaguar','stabalize','destabilize','democratic','kingdoms','dramatic','favourable']\n",
        "print(bundle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZNuWYLFe_AC",
        "outputId": "c99eb11f-07c7-4fd4-c7ec-174573ea9561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word                Porter              Lancaster           Snowball            \n",
            "bunnies             bunni               bunny               bunni               \n",
            "organization        organ               org                 organ               \n",
            "polarize            polar               pol                 polar               \n",
            "jaguar              jaguar              jagu                jaguar              \n",
            "stabalize           stabal              stab                stabal              \n",
            "destabilize         destabil            dest                destabil            \n",
            "democratic          democrat            democr              democrat            \n",
            "kingdoms            kingdom             kingdom             kingdom             \n",
            "dramatic            dramat              dram                dramat              \n",
            "favourable          favour              favo                favour              \n"
          ]
        }
      ],
      "source": [
        "#structure of columns\n",
        "#20 is the padding\n",
        "print(\"{0:20}{1:20}{2:20}{3:20}\".format(\"Word\",\"Porter\",\"Lancaster\",\"Snowball\"))   \n",
        "\n",
        "for word in bundle:\n",
        "  print(\"{0:20}{1:20}{2:20}{3:20}\".format(word,porter.stem(word),lancaster.stem(word),snow.stem(word)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3JiagalgGqI"
      },
      "source": [
        "We can see that : <br>\n",
        "  Lancaster is more aggressive. <br>\n",
        "  It overiterates which can strip suffixes and create a word which linguistically does not make much sense.<br>\n",
        "  Overstemming.\n",
        "\n",
        "Thus, Porter and Snowball are more famous as the stems they give make much more sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZX_dnIHlI2f"
      },
      "source": [
        "# <font color='#fd79a8'>  Lemmatization  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS6cRODolK-p"
      },
      "source": [
        "### Libraries used : NLTK's Wordnet, spaCy, TextBlob, Pattern & Standford Core NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCVtwaCCfljd",
        "outputId": "41ebf49f-8a55-4f7e-946c-2d930404cbd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk   #library\n",
        "from nltk.stem import WordNetLemmatizer    #package\n",
        "nltk.download('wordnet')       #wordnet is a lexicon with nouns, verbs and so on\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbSW2zjGl-Ua"
      },
      "source": [
        "[Wordnet Resource Search](http://wordnetweb.princeton.edu/perl/webwn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4rVmKVullh9",
        "outputId": "0fd2650f-1864-4804-f4ed-c62d70c2b0c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stockings  =>  stocking\n"
          ]
        }
      ],
      "source": [
        "#creating an object of WordNetLemmatizer class.\n",
        "lem = WordNetLemmatizer()\n",
        "token = \"stockings\"\n",
        "\n",
        "result_lemma = lem.lemmatize(token)\n",
        "print(token,\" => \", result_lemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F4NbM5pVmf5d",
        "outputId": "cc767de5-a77e-4e4e-b722-a2631e9ef4b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'stock'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "porter.stem(\"stockings\")        #incorrect stemming method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRyc3U_rnLo9"
      },
      "source": [
        "Lemmatizer uses the wordnet to figure out the word and gives us it's noun form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuks1KounhlQ"
      },
      "source": [
        "### Lemmatizing an entire sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs3HMV1pmnbZ"
      },
      "outputs": [],
      "source": [
        "string1 = \"The girls sang louder. The bankers banked at other banks.\" \n",
        "string2 = \"These were better shoes for her feet. The grocer was stocking the shelves at the grocery\" \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPpqd0tanync"
      },
      "source": [
        "Tokenizing sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWklBAWFnnXZ",
        "outputId": "144943d5-8e7d-41be-8051-8750856b99d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BufzOm6OnwZ-",
        "outputId": "23003092-9344-4120-d2aa-8d550bd1cc8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Sentence : \n",
            "['The', 'girls', 'sang', 'louder', '.', 'The', 'bankers', 'banked', 'at', 'other', 'banks', '.']\n",
            "Lemmatized Sentence :\n",
            " The girl sang louder . The banker banked at other bank .\n"
          ]
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize(string1)\n",
        "print(\"Tokenized Sentence : \")\n",
        "print(tokens)\n",
        "\n",
        "#Lemmatize the tokenized sentences\n",
        "lemmatized_tokens = ' '.join(lem.lemmatize(w) for w in tokens)\n",
        "print(\"\\nLemmatized Sentence :\\n\",lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm9-PImnn3ZB",
        "outputId": "4e83a3e7-d677-4082-8bc7-9c94740b8841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lemmatized Sentence :\n",
            " T h e   g i r l s   s a n g   l o u d e r .   T h e   b a n k e r s   b a n k e d   a t   o t h e r   b a n k s .\n"
          ]
        }
      ],
      "source": [
        "#lemmatizing without tokens\n",
        "lemmatized_tokens = ' '.join(lem.lemmatize(w) for w in string1)\n",
        "print(\"\\nLemmatized Sentence :\\n\",lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3anNTNkokoJ"
      },
      "source": [
        "We can see that the accuracy is much better when we use tokens instead of sentence directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55qdYoSroinp",
        "outputId": "e54b4bce-bd5c-49b0-d88c-0b9241fb7511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lemmatized Sentence :\n",
            " The girl sang louder. The banker banked at other banks.\n"
          ]
        }
      ],
      "source": [
        "lemmatized_tokens = ' '.join(lem.lemmatize(word) for word in string1.split())\n",
        "print(\"\\nLemmatized Sentence :\\n\",lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7i2ZoGpKS6"
      },
      "source": [
        "Split method also gives us tokens but we not that it doesn't give us expanded clitics which we'd want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J84fc5E8o9MN",
        "outputId": "ebe92469-333c-4c27-900d-8e765412b043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lemmatized Sentence :\n",
            " Split method also give u token but we know that it doesn't give u expanded clitics which we'd want\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Split method also gives us tokens but we know that it doesn't give us expanded clitics which we'd want\"\n",
        "\n",
        "lemmatized_tokens = ' '.join(lem.lemmatize(word) for word in sentence.split())\n",
        "print(\"\\nLemmatized Sentence :\\n\",lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2sgMZqsphah"
      },
      "source": [
        "Thus, Lemmatization after Tokenization is to be done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ25n_E5paAx",
        "outputId": "647e2b91-eabf-4704-9b5a-1d99f9f6c0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Sentence : \n",
            "['The', 'girls', 'sang', 'louder', '.', 'The', 'bankers', 'banked', 'at', 'other', 'banks', '.']\n",
            "\n",
            "Lemmatized Sentence :\n",
            " The girl sang louder . The banker banked at other bank .\n"
          ]
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize(string1)\n",
        "print(\"Tokenized Sentence : \")\n",
        "print(tokens)\n",
        "\n",
        "#Lemmatize the tokenized sentences\n",
        "lemmatized_tokens = ' '.join(lem.lemmatize(w) for w in tokens)\n",
        "print(\"\\nLemmatized Sentence :\\n\",lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnqTdXBOp1QK"
      },
      "source": [
        "Problem :<br>\n",
        "should give us verb of sang (sing) as the lemma.\n",
        "<br>\n",
        "also, banked is a verb whose root lemma is bank. \n",
        "\n",
        "Thus, accuracy in terms of verbs is not good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkL-Y1D5p0QO",
        "outputId": "5b23506f-0e46-4024-dee6-4ed725c0bf23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Sentence : \n",
            "['These', 'were', 'better', 'shoes', 'for', 'her', 'feet', '.', 'The', 'grocer', 'was', 'stocking', 'the', 'shelves', 'at', 'the', 'grocery']\n",
            "\n",
            "Lemmatized Sentence :\n",
            " These were better shoe for her foot . The grocer wa stocking the shelf at the grocery\n"
          ]
        }
      ],
      "source": [
        "tokens2 = nltk.word_tokenize(string2)\n",
        "print(\"Tokenized Sentence : \")\n",
        "print(tokens2)\n",
        "\n",
        "#Lemmatize the tokenized sentences\n",
        "lemmatized_tokens2 = ' '.join(lem.lemmatize(w) for w in tokens2)\n",
        "print(\"\\nLemmatized Sentence :\\n\",lemmatized_tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qDzpp5nrUbi"
      },
      "source": [
        "Problem:<br>\n",
        "were => were, correct lemma(be ,verb)\n",
        "was => wa , correct lemma(be ,verb)\n",
        "stocking => stocking, correct lemma(stock, verb)\n",
        "\n",
        "<br>\n",
        "Precised Output:\n",
        "<br>These be better shoe for her foot.\n",
        "<br>The grocer be stock the shelf at the grocery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK-4NRNpr5mn"
      },
      "source": [
        "The algorithm has to consider the part of speech before lemmatizing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxcav_DRsCV7"
      },
      "source": [
        "<font color='#fd79a8'> A second argument must be added to lemmatize() - to include (or to tag) the Parts-of-Speech. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkgCdWyarMIm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}