{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxOkcvB9sqy6"
      },
      "source": [
        "<font color=\"#00d2d3\">Build Vocabulary - aka Preprocessing Words into Cleaned Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVQ6RanQYPdp"
      },
      "source": [
        "#<font color='#4cd137'> Bag oF Words Architecture: <br>Tokenize - Create Vocab - Count Frequency of Tokens - Create Vectorized Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMAea-PalPd4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Tb74Wohlhpo"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grNCis75luy2"
      },
      "outputs": [],
      "source": [
        "#picture that each sentence is a document.\n",
        "#each entry of list is a token\n",
        "text = [\n",
        "    \"A rose is a flower\",\n",
        "    \"A fern is not a flower\",\n",
        "    \"the dof are the rose\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgcOs6lEpqpp",
        "outputId": "06fb447d-b54b-45be-a3af-d271d1f5db58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQC-5EYpjtj_"
      },
      "source": [
        "[Keras Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)<br> <font color='#4cd137'>tf.keras.preprocessing.text.Tokenizer \n",
        "<br> method fit_on_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHzcf-YSgJpt"
      },
      "outputs": [],
      "source": [
        "model = Tokenizer()\n",
        "# it updates the entire vocab based on the above mentioned text\n",
        "#we don't need to convert it to pandas series cuz fit on text() can work on list of texts\n",
        "model.fit_on_texts(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5veICZSjjYxb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sqD2CfZgJvO",
        "outputId": "d9a34bd6-6668-47be-e008-d9c3938631d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'rose', 'is', 'flower', 'the', 'fern', 'not', 'dof', 'are']\n"
          ]
        }
      ],
      "source": [
        "print(list(model.word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiKSiVEBpnZp",
        "outputId": "b6acf2eb-9ebd-41c0-d0f9-72413f926412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list(model.word_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG68ZM5BqR1o"
      },
      "source": [
        "<font color='#4cd137'> text_to_matrix <br> numpy array with the shape len sequences * size vocab (or no of tokens in the vocab)\n",
        "\n",
        "3 x 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auiBpKQqpybq",
        "outputId": "e1dc7081-f8d0-4124-df2d-eed5ae3345d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 2., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 2., 0., 1., 1., 0., 1., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 2., 0., 0., 1., 1.]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec_rep = model.texts_to_matrix(text,mode='count')   #count can also be tfidf\n",
        "vec_rep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IETZjEmWhL98"
      },
      "source": [
        "<font color='#4cd137'>The length of the vector will always be equal to \n",
        "\n",
        "---\n",
        "\n",
        "vocabulary size * len document"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}