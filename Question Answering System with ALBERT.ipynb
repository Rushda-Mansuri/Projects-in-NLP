{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg5cvfF1ODpY"
      },
      "source": [
        "<font color=\"#ffb142\">Reading Comprehension Overview<br><font color=\"#636e72\">Venezuela, officially the Bolivarian Republic of Venezuela (Spanish: República Bolivariana de Venezuela),[12] is a country on the northern coast of South America, consisting of a continental landmass and many islands and islets in the Caribbean Sea. It has a territorial extension of 916,445 km2 (353,841 sq mi), and the population of Venezuela was estimated at 28 million in 2019.[7] The capital and largest urban agglomeration is the city of Caracas.<br>\n",
        "\n",
        "\n",
        "*   How many people live in Venezuela?\n",
        "\n",
        "*   Q&A Response: <br>\n",
        "28 million\n",
        "\n",
        "<font color=\"#fd79a8\">Pre-Trained Transformer Model - ALBERT. <font color=\"#636e72\">It is said that ALBERT has shorter training and inference times than BERT. We would be fine tuning ALBERT for our Q&A task. <br>\n",
        "<font color=\"#fd79a8\"> Transformer Library: Huggings Face\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLoAxFPDQzb9"
      },
      "source": [
        "<font color=\"#ffb142\">Check GPU Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9rXQMfKh6AC",
        "outputId": "f929480f-7a11-4852-8aea-709bfdf3bc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jan  6 09:59:17 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    31W /  70W |    312MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ytxxwJliEEt",
        "outputId": "63023f5e-bac2-40bb-9569-702be78c8840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUwUowlUSxIp"
      },
      "source": [
        "<font color=\"#ffb142\"> Source Hugging Face Library from Github<br>Git checkout: Switch branches or restore working tree files<br>[Git Hub Repo](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCjgXDzBnjsV",
        "outputId": "7e33e64c-d31e-40a1-f39d-0758a0e392d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path ' ' already exists and is not an empty directory.\n",
            "/bin/bash: line 0: cd: transformers: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers \\  \n",
        "!cd transformers \\\n",
        "&& git checkout a3085020ed0d81d4903c50967687192e3101e770 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxB_V_Slnjxw",
        "outputId": "8200ba7e-ffa0-4095-b388-74970235603c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Invalid requirement: './transformers/'\n",
            "Hint: It looks like a path. File './transformers/' does not exist.\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install ./transformers/\n",
        "!pip install tensorboardX\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSGV9mBZT5ov"
      },
      "source": [
        "<font color=\"#ffb142\"> Get Training Data SQuAD 2.0 <br>[SQuAD Resource](https://rajpurkar.github.io/SQuAD-explorer/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh5jDQEqnoxu",
        "outputId": "265f5641-df72-4344-f996-1eff69d04008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-06 09:48:09--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M   256MB/s    in 0.2s    \n",
            "\n",
            "2023-01-06 09:48:10 (256 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2023-01-06 09:48:10--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-01-06 09:48:10 (76.4 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir dataset \\\n",
        "&& cd dataset \\\n",
        "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
        "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJCVBXWpUw3Z"
      },
      "source": [
        "<font color=\"#ffb142\">  ALBERT: Train Model <br>[Find Model Name](https://huggingface.co/transformers/pretrained_models.html) <br> [ALBERT Parameters](https://huggingface.co/transformers/model_doc/bert.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtLdGj8Jny7m",
        "outputId": "4aecde9f-16f7-41f8-f6ed-51bca368bc1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file 'transformers/examples/run_squad_trainer.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!export SQUAD_DIR=/content/dataset \\\n",
        "&& python transformers/examples/run_squad_trainer.py \\\n",
        "  --model_type albert \\\n",
        "  --model_name_or_path albert-base-v2 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --train_file $SQUAD_DIR/train-v2.0.json \\\n",
        "  --predict_file $SQUAD_DIR/dev-v2.0.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 1.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir /content/model_output \\\n",
        "  --save_steps 1000 \\\n",
        "  --threads 2 \\\n",
        "  --version_2_with_negative \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoXIzq9YrFGq"
      },
      "source": [
        "\n",
        "<font color=\"#fd79a8\"><br> [SentencePiece](https://github.com/google/sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gbXU7SalOIV"
      },
      "source": [
        "<font color=\"#ffb142\">model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-RXQHd8nzBX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import time \n",
        "\n",
        "from torch.utils.data import DataLoader \n",
        "\n",
        "from transformers import (\n",
        "    AlbertConfig, \n",
        "    AlbertForQuestionAnswering,\n",
        "    AlbertTokenizer,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "\n",
        "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
        "\n",
        "use_own_model = False\n",
        "\n",
        "if use_own_model:\n",
        "  model_name_or_path = \"/content/model_output\"\n",
        "else:\n",
        "  model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
        "\n",
        "output_dir = \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh8L88dq4QUG"
      },
      "source": [
        "<font color=\"#fd79a8\"> Configure QA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMui0FBOAZ_n"
      },
      "outputs": [],
      "source": [
        "#no of predictions we want, per question\n",
        "n_best_size = 1  \n",
        "max_answer_length = 30\n",
        "do_lower_case = True \n",
        "#an unanswerable question will be null\n",
        "null_score_diff_threshold = 0.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Q-NUBA9hIm"
      },
      "source": [
        "<font color=\"#ffb142\"> Setup Model & Tensor Attributes\n",
        "\n",
        "tensor.detach() : returns new tensor\n",
        "and then we present it in form of list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27sZtUEC-EjT"
      },
      "outputs": [],
      "source": [
        "def to_list(tensor):\n",
        "  return tensor.detach().cpu().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "hLSU1F5T_JAC",
        "outputId": "bcbacc3b-3183-4bf8-f848-463d30e43b37"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cd61c587082f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ],
      "source": [
        "config_class, model_class, tokenizer_class = (\n",
        "    AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer\n",
        ")\n",
        "config = config_class.from_pretrained(model_name_or_path)\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True)\n",
        "\n",
        "model = model_class.from_pretrained(model_name_or_path, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK8MQUhoAbAo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "processor = SquadV2Processor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck6uq-poDxL0"
      },
      "source": [
        "<font color=\"#ffb142\">class SquadExample:<br>\n",
        "    \"\"\"<br>\n",
        "    <font color=\"#fd79a8\"> A single training/test example for the Squad dataset, as loaded from disk.\n",
        "    Args:<br>\n",
        "        qas_id: The example's unique identifier<br>\n",
        "        question_text: The question string<br>\n",
        "        context_text: The context string<br>\n",
        "        answer_text: The answer string<br>\n",
        "        start_position_character: The character position of the start of the <br>answer<br>\n",
        "        title: The title of the example<br>\n",
        "        answers: None by default, this is used during evaluation. Holds answers as well as their start positions.<br>\n",
        "        is_impossible: False by default, set to True if the example has no possible answer.<br>\n",
        "    \"\"\"\n",
        "    [Pytorch Documentation](https://pytorch.org/docs/stable/data.html)<br> [Model Outputs Documentation](https://huggingface.co/transformers/main_classes/output.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SD_rCf4EtA9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import (\n",
        "    AlbertConfig,\n",
        "    AlbertForQuestionAnswering,\n",
        "    AlbertTokenizer,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "\n",
        "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
        "\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
        "\n",
        "# READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n",
        "use_own_model = False\n",
        "\n",
        "if use_own_model:\n",
        "  model_name_or_path = \"/content/model_output\"\n",
        "else:\n",
        "  model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
        "\n",
        "output_dir = \"\"\n",
        "\n",
        "#Configuring A Question Answer Model\n",
        "# Config\n",
        "#The number of predictions given per question.\n",
        "n_best_size = 1\n",
        "#The maximum token length of an answer that can be generated.\n",
        "max_answer_length = 30\n",
        "do_lower_case = True\n",
        "#If (null_score - best_non_null) is greater than the threshold predict null.\n",
        "null_score_diff_threshold = 0.0\n",
        "\n",
        "#Returns a new Tensor\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "# Setup model\n",
        "config_class, model_class, tokenizer_class = (\n",
        "    AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
        "config = config_class.from_pretrained(model_name_or_path)\n",
        "tokenizer = tokenizer_class.from_pretrained(\n",
        "    model_name_or_path, do_lower_case=True)\n",
        "model = model_class.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "#tensor attributes \n",
        "#A torch.device is an object representing the device on which a torch.Tensor is or will be allocated\n",
        "#the Tensor.device property allows a torch.Tensor device to be accessed\n",
        "#The torch.device contains a device type either ('cpu' or 'cuda') \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "processor = SquadV2Processor()\n",
        "\n",
        "def run_prediction(question_texts, context_text):\n",
        "    \"\"\"Setup function to compute predictions\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    for i, question_text in enumerate(question_texts):\n",
        "        example = SquadExample(\n",
        "            qas_id=str(i),\n",
        "            question_text=question_text,\n",
        "            context_text=context_text,\n",
        "            answer_text=None,\n",
        "            start_position_character=None,\n",
        "            title=\"Predict\",\n",
        "            is_impossible=False,\n",
        "            answers=None,\n",
        "        )\n",
        "\n",
        "        examples.append(example)\n",
        "\n",
        "    features, dataset = squad_convert_examples_to_features(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=384,\n",
        "        doc_stride=128,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        return_dataset=\"pt\",\n",
        "        threads=1,\n",
        "    )\n",
        "\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            for i, example_index in enumerate(example_indices):\n",
        "                eval_feature = features[example_index.item()]\n",
        "                unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "                output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "                all_results.append(result)\n",
        "\n",
        "    output_prediction_file = \"predictions.json\"\n",
        "    output_nbest_file = \"nbest_predictions.json\"\n",
        "    output_null_log_odds_file = \"null_predictions.json\"\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        n_best_size,\n",
        "        max_answer_length,\n",
        "        do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        False,  # verbose_logging\n",
        "        True,  # version_2_with_negative\n",
        "        null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-_wlfXVbaNR"
      },
      "source": [
        "##<font color=\"#ffb142\"> Try out our Q&A System!!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJYC3Q77bftT",
        "outputId": "98b1c9ef-1b52-4bed-a4bd-c21ae0ab0174"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 90.85it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 10381.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "J.Lo,\n"
          ]
        }
      ],
      "source": [
        "# context = \"\"\"Venezuela, officially the Bolivarian Republic of Venezuela is a country on the northern coast of South America, consisting of a continental landmass and many islands and islets in the Caribbean Sea. It has a territorial extension of 916,445 km2 (353,841 sq mi), and the population of Venezuela was estimated at 28 million in 2019.[7] The capital and largest urban agglomeration is the city of Caracas.\"\"\"\n",
        "# questions = [\"How many people live in venezuela\"]\n",
        "\n",
        "# context = \"\"\"New Zealand (Māori: Aotearoa) is a sovereign island country in the southwestern Pacific Ocean. It has a total land area of 268,000 square kilometres (103,500 sq mi), and a population of 4.9 million. New Zealand's capital city is Wellington, and its most populous city is Auckland.\"\"\"\n",
        "# questions = [\"How many people live in New Zealand?\",\n",
        "#              \"What's the largest city?\" ]\n",
        "\n",
        "context = \"\"\"Jennifer Lynn Lopez[2] (born July 24, 1969), also known by her nickname J.Lo, is an American singer, actress and dancer. In 1991, Lopez began appearing as a Fly Girl dancer on In Living Color, where she remained a regular until she decided to pursue an acting career in 1993. For her first leading role in the 1997 Selena biopic of the same name, Lopez became the first Latin actress to earn over US$1 million for a film. She went on to star in Anaconda (1997) and Out of Sight (1998), and has subsequently established herself as one of the highest-paid Latin actresses worldwide as of 2017.[3] Lopez is considered a pop culture icon, and is often described as a triple threat entertainer.[4][5][6]\n",
        "\n",
        "Lopez ventured into the music industry with her debut studio album On the 6 (1999), which helped propel the Latin pop movement in American music. With the simultaneous release of her second studio album J.Lo and her romantic comedy The Wedding Planner in 2001, Lopez became the first woman to have a number-one album and film in the same week. Her 2002 release, J to tha L–O! The Remixes, became the first remix album in history to debut atop the US Billboard 200. Later that year, she released her third studio album This Is Me... Then and starred in the film Maid in Manhattan.\"\"\"\n",
        "questions = [\"What is the nickname of Jennifer Lopez?\"]\n",
        "         \n",
        "\n",
        "\n",
        "predictions = run_prediction(questions, context)\n",
        "\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_IBDvOglyRw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}